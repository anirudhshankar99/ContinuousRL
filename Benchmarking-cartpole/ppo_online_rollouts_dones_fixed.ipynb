{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils import tensorboard\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mish(input):\n",
    "    return input * torch.tanh(F.softplus(input))\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def __init__(self): super().__init__()\n",
    "    def forward(self, input): return mish(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to convert numpy arrays to tensors\n",
    "def t(x): return torch.from_numpy(x).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor module, categorical actions only\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, n_actions, activation=nn.Tanh):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            activation(),\n",
    "            nn.Linear(64, 32),\n",
    "            activation(),\n",
    "            nn.Linear(32, n_actions),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic module\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, activation=nn.Tanh):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            activation(),\n",
    "            nn.Linear(64, 32),\n",
    "            activation(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# config\n",
    "state_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "actor = Actor(state_dim, n_actions, activation=Mish)\n",
    "critic = Critic(state_dim, activation=Mish)\n",
    "adam_actor = torch.optim.Adam(actor.parameters(), lr=3e-4)\n",
    "adam_critic = torch.optim.Adam(critic.parameters(), lr=1e-3)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "env.action_space.seed(1)\n",
    "env.observation_space.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_grad_norm_(module, max_grad_norm):\n",
    "    nn.utils.clip_grad_norm_([p for g in module.param_groups for p in g[\"params\"]], max_grad_norm)\n",
    "\n",
    "def policy_loss(old_log_prob, log_prob, advantage, eps):\n",
    "    ratio = (log_prob - old_log_prob).exp()\n",
    "    clipped = torch.clamp(ratio, 1-eps, 1+eps)*advantage\n",
    "    \n",
    "    m = torch.min(ratio*advantage, clipped)\n",
    "    return -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8000 [00:00<?, ?it/s]/home/a.shankar/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1739: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      " 33%|███▎      | 2621/8000 [18:04<2:24:57,  1.62s/it]"
     ]
    }
   ],
   "source": [
    "episode_rewards = []\n",
    "gamma = 0.98\n",
    "eps = 0.2\n",
    "log = True\n",
    "if log:\n",
    "    w = tensorboard.SummaryWriter('../runs/rollout_dones_fixed')\n",
    "s = 0\n",
    "max_grad_norm = 0.5\n",
    "total_timesteps = 8000\n",
    "episode_length = 2048\n",
    "for i in tqdm(range(int(total_timesteps))):\n",
    "    prev_prob_act = None\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    observations, actions, logprobs, rewards, dones, values = torch.zeros((episode_length,)+env.observation_space.shape, dtype=torch.float32), torch.zeros((episode_length,)+env.action_space.shape, dtype=torch.float32), torch.zeros((episode_length,), dtype=torch.float32), torch.zeros((episode_length,), dtype=torch.float32), torch.zeros((episode_length,), dtype=torch.float32), torch.zeros((episode_length,), dtype=torch.float32)\n",
    "    j = 0\n",
    "    while not done and j < episode_length:\n",
    "        # gathering rollout data\n",
    "        s += 1\n",
    "        with torch.no_grad():\n",
    "            probs = actor(torch.from_numpy(state).float())\n",
    "        value = critic(torch.from_numpy(state).float())\n",
    "        dist = torch.distributions.Categorical(probs=probs)\n",
    "        action = dist.sample()\n",
    "        prob_act = dist.log_prob(action)\n",
    "        \n",
    "        observations[j] = torch.from_numpy(state).float()\n",
    "        actions[j] = action\n",
    "        logprobs[j] = prob_act\n",
    "        next_state, reward, done, _, info = env.step(action.detach().numpy())\n",
    "        rewards[j] = reward\n",
    "        dones[j] = done\n",
    "        values[j] = value\n",
    "        state = next_state\n",
    "        j += 1\n",
    "    \n",
    "    # if (episode_length - j)/episode_length < 0.05: episode_length *=2 \n",
    "    # print(f'Rollout ended with done={done}, reward={rewards.sum().numpy()}')\n",
    "    # with torch.no_grad():\n",
    "    # advantage calculation\n",
    "    advantages = torch.zeros((episode_length,), dtype=torch.float32)\n",
    "    # advantage = reward + (1-done)*gamma*critic(t(next_state)) - critic(t(state))\n",
    "    done_index = dones.nonzero().max().item() if dones.any() else episode_length\n",
    "    for t in reversed(range(done_index)):\n",
    "        if t == episode_length - 1:\n",
    "            advantages[t] = rewards[t] + (1-dones[t])*gamma*values[t]\n",
    "        else:\n",
    "            advantages[t] = rewards[t] + (1-dones[t+1])*gamma*(values[t+1] - values[t])\n",
    "    \n",
    "    if log:\n",
    "        w.add_scalar(\"loss/advantage\", advantages.detach().numpy().mean(), global_step=i)\n",
    "        # w.add_scalar(\"actions/action_0_prob\", dist.probs[0], global_step=s)\n",
    "        # w.add_scalar(\"actions/action_1_prob\", dist.probs[1], global_step=s)\n",
    "    \n",
    "    total_reward = rewards.sum().numpy()\n",
    "    if log:\n",
    "        w.add_scalar(\"reward/episode_reward\", total_reward, global_step=i)\n",
    "\n",
    "    probs = actor(observations)\n",
    "    dist = torch.distributions.Categorical(probs=probs)\n",
    "    new_logprobs = dist.log_prob(actions)\n",
    "    actor_loss = policy_loss(logprobs, new_logprobs, advantages.detach(), eps).mean()\n",
    "    adam_actor.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    if log:\n",
    "        w.add_scalar(\"loss/actor_loss\", actor_loss.detach(), global_step=i)\n",
    "        w.add_histogram(\"gradients/actor\",\n",
    "                        torch.cat([p.grad.view(-1) for p in actor.parameters()]), global_step=i)\n",
    "    adam_actor.step()\n",
    "    state = next_state\n",
    "\n",
    "    critic_loss = advantages.pow(2).mean()\n",
    "    adam_critic.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    if log:\n",
    "        w.add_scalar(\"loss/critic_loss\", critic_loss.detach(), global_step=j)\n",
    "        w.add_histogram(\"gradients/critic\",\n",
    "                        torch.cat([p.grad.view(-1) for p in critic.parameters()]), global_step=i)\n",
    "    adam_critic.step()\n",
    "\n",
    "    episode_rewards.append(total_reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
